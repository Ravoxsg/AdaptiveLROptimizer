\documentclass{article}
  
  % if you need to pass options to natbib, use, e.g.:
  % \PassOptionsToPackage{numbers, compress}{natbib}
  % before loading nips_2016
  %
  % to avoid loading the natbib package, add option nonatbib:
  % \usepackage[nonatbib]{nips_2016}
  
  %\usepackage{nips_2016}
  
  % to compile a camera-ready version, add the [final] option, e.g.:
  \usepackage[final]{nips_2016}
  
  \usepackage[utf8]{inputenc} % allow utf-8 input
  \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
  \usepackage{hyperref}       % hyperlinks
  \usepackage{url}            % simple URL typesetting
  \usepackage{booktabs}       % professional-quality tables
  \usepackage{amsfonts}       % blackboard math symbols
  \usepackage{nicefrac}       % compact symbols for 1/2, etc.
  \usepackage{microtype}      % microtypography
  \usepackage{indentfirst}
  \usepackage{amsmath}
  \usepackage{fancyhdr}
  
  \fancypagestyle{equalc}{\fancyhf{}\renewcommand{\headrulewidth}{0pt}\fancyfoot[R]{* indicates equal contribution}}
  
  \title{Faster gradient descent convergence via an adaptive learning rate schedule}
  
  % The \author macro works with any number of authors. There are two
  % commands used to separate the names and addresses of multiple
  % authors: \And and \AND.
  %
  % Using \And between authors leaves it to LaTeX to determine where to
  % break the lines. Using \AND forces a line break at that point. So,
  % if LaTeX puts 3 of 4 authors names on the first line, and the last
  % on the second line, try using \AND instead of \And before the third
  % author name.
  
  \author{
  	Satya Krishna Gorti*\\
  	University of Toronto\\
  	27 King's College Circle\\
  	Toronto, ON M5S\\
  	satyag@cs.toronto.edu
  	\And
	Mathieu Ravaut* \\ 
	University of Toronto \\
	27 King's College Circle\\
	Toronto, ON M5S\\
	mravox@cs.toronto.edu \\
}
  
  \begin{document}
  % \nipsfinalcopy is no longer used
  
  \maketitle
  
  \begin{abstract}
    Any gradient descent requires to choose a learning rate. With deeper and deeper models, tuning that learning rate can easily become tedious and does not necesarily lead to an ideal convergence. We propose a variation of the gradient descent algorithm in the which the learning rate $\eta$ is not fixed. Instead, we learn $\eta$ itself, either by another gradient descent (first-order method), or by Newton's method (second-order). That way, gradient descent for any machine learning algorithm can be optimized. 
  \end{abstract}
  
  \thispagestyle{equalc}
  \section{Introduction}
  
  In the past decades, gradient descent has been widely adopted to optimize the loss function in machine learning algorithms \emph{ref}. Lately, the machine learning community has also used the stochastic gradient descent alternative \emph{ref}. Gradient descent can be used in any dimension, and presents the advantage of being easy to understand and inexpensive to compute. Under certain assumptions on the loss function (such as convexity), gradient descent is guaranteed to converge to the minimum of the function. However, stochastic gradient descent has proven to be very efficient even in situations where the loss functions is not convex, as is mostly the case with modern deep neural networks \emph{ref}. Other methods such as Newton's method guarantee a much faster convergence, but are typicaaly very expensive. Newton's method for instance requires to compute the inverse of the Hessian matrix of the loss functions with regards to all parameters, which is impossible with today's hardware and today's deep networks with millions of parameters \emph{ref}. \\
  
  The quality of a gradient descent heavily depends on the choice of the learning rate $\eta$. A too high learning rate will see the loss function jumping around the direction of steepest descent, and eventually diverge. While a very low learning rate guarantees non-divergence, convergence will be very slow, and the loss function might get stuck in a local minimum. Choosing an ideal learning rate requires an intuition of the problem. Typically, researchers would start by performing a line-search over a set of different orders of magnitude of learning rates, but this is long and costly. Besides, a line-search usually assumes a fixed learning rate over time, as doing one line-search per iteration would require exponential computation cost. Usually, we see researchers setting the learning rate to an initial value then decrasing it one of a few items after training has progressed \emph{ref}.\\
  
  In this paper, we propose to automatically find the learning rate. We still need to input an initial value, but at each iteration, our model will find a learning that optimizes best the loss function at this point of learning. We explore a first-order method and a second-order method to do so, with a strong emphasis on the latter. Our method could be applied to any machine learning algorithm using gradient descent. We show faster convergence on a variety of tasks and models. \\ 
  
  \section{Related work}
  
  \section{Adaptive learning rate}
  
  An adaptive learning gradient descent has the following schedule:\\
  \begin{equation}
  w(t+1) = w(t) -\eta(t)g(t)\\
  \end{equation}
  where $g(t) = \nabla L(w(t))$, L is the loss function, and w(t) represents the state of the model's weights at time t. In the following we present several ways to update $\eta(t)$.
  
  \subsection{First-order method}
  
  The first-order method consists in doing gradient descent on the learning rate. Let's introduce a function that will be useful in the following:\\
  \begin{align}
  f : R^{n} &\rightarrow R\\
  \eta &\rightarrow L(w(t)-\eta g(t))
  \end{align}
  n corresponds to the number of learnable parameters in the model. f represents how the loss would be if we were to perform a gradient descent update with the given $\eta$.\\
  The first-order method is written:\\
  \begin{align}
  w(t+1) &= w(t) -\eta(t)g(t)\\
  \eta(t+1) &= \eta(t) - \alpha f'(t)
  \end{align} 
  This method introduces a new "meta" learning rate $\alpha$. It has the advantage of begin light in cost as we only need gradients of f. Indeed:\\
  \begin{equation}
  \forall \eta, f'(\eta) = -g(t)^{T}.\nabla L(w(t)-\eta g(t)) \text{ where . is the dot product in dimension n}
  \end{equation}
  In particular, at the value of $\eta$ used to get w(t+1), we get:\\
  \begin{equation}
  f'(\eta(t)) = -g(t)^{T}.g(t+1)
  \end{equation}
  
  \subsection{Second-order method}
  
  The previous method transfers the problem of choosing an ideal learning rate for weights to choosing an ideal learning rate for the learning rate itself. To avoid that problem, the second-order method uses a Newton-Raphson algorithm:\\
  \begin{align}  
  w(t+1) &= w(t) -\eta(t)g(t)\\
  \eta(t+1) &= \eta(t) - \frac{f'(\eta(t))}{f''(\eta(t))}
  \end{align}
  Now all the learning only depends on the loss. However, the second derivative of requires building its Hessian matrix:\\
  \begin{equation}
  \forall \eta, f''(\eta) = g(t)^{T}.H_{L}(w(t)-\eta g(t))
  \end{equation}
  We propose to approximate this Hessian using finite differences. By using on f' then f'', we get an update formula of the learning rate depending only on several values of the loss function:\\
  \begin{equation}
  f'(\eta+\epsilon) \approx \frac{f(\eta + 2\epsilon)-f(\eta)}{2\epsilon}
  \end{equation}
  \begin{equation}
  \text{and }f'(\eta-\epsilon) \approx \frac{f(\eta)-f(\eta-2\epsilon)}{2\epsilon}
  \end{equation}  
  \begin{equation}
  \text{so }f''(\eta) \approx \frac{f(\eta+2\epsilon)+f(\eta-2\epsilon)-2f(\eta)}{4 \epsilon^{2}}
  \end{equation}
  Given the finite differences on the first derivative:\\    
  \begin{equation}
  f'(\eta) \approx \frac{f(\eta+\epsilon)-f(\eta-\epsilon)}{2 \epsilon}
  \end{equation}
  We get the final simple formula for $\eta$:\\
  \begin{equation}
  \eta(t+1) = \eta(t) - 2\epsilon\frac{(f(\eta+\epsilon)-f(\eta-\epsilon))}{f(\eta+2\epsilon)+f(\eta-2\epsilon)-2f(\eta)}
  \end{equation}    
  
  
  
  \subsection{BFGS}
  
  \section{Experiments}
  
  \subsection{Linear regression}
  
  \subsection{Image classification with neural networks}
  
  \subsection{Logistic regression}
  
  \section{Further exploration}
  
  \subsection{Learning the learning rate}
  
  \subsection{Momentum}
  
  \subsection{Getting loss values via a validation set}
  
  \section{Limitations}
  
  \subsection{Choice of step $\epsilon$}
  
  \subsection{Choice of the initial learning rate}
  
  \subsection{Cost of loss computations}
  
  \section{Conclusion}
  
  
  \section*{References}
  
  \medskip
  
  \small
  
  [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
  for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
  T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
    Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.
  
  [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
    Exploring Realistic Neural Models with the GEneral NEural SImulation
    System.}  New York: TELOS/Springer--Verlag.
  
  [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
  learning and recall at excitatory recurrent synapses and cholinergic
  modulation in rat hippocampal region CA3. {\it Journal of
    Neuroscience} {\bf 15}(7):5249-5262.
  
  \end{document}